# Wally

> [!WARNING]
> Almost every character in this repository has been generated by an LLM.

Wally is a robot from the future that is responsible with handling the AI garbage.

## Overview

One of the more revolutionary ideas involving LLMs was [Geoffrey Huntley's "Ralph Wiggum as a software engineer"][ralph]
approach, which boils down to running the following process in a loop:

1. The LLM is given a task from an implementation plan
2. The LLM implements the task
3. The LLM updates the implementation plan

The loop stops when there are no tasks for the LLM to implement.

This approach can be very useful when working with a piece of software that does not have good test coverage.
In this case, the loop will become:

1. Implement the next [characterization test][char-test] based on a list of tests to be implemented
2. Run tests using a code coverage tool like [SimpleCov] or [mutant]
3. Update the list of tests to be done

[ralph]: https://ghuntley.com/ralph/
[char-test]: https://michaelfeathers.silvrback.com/characterization-testing
[SimpleCov]: https://github.com/simplecov-ruby/simplecov
[mutant]: https://github.com/mbj/mutant/tree/main

The original implementation used a basic bash loop, but more modern AI agents like [opencode] use a client-server
architecture which allows more fine-grained control over an API. So, in a sense, `Wally` is an orchestrator
for `opencode`.

[opencode]: https://opencode.ai/

## Usage

Start a headless `opencode` server:

```bash
opencode serve
```

Then run Wally

```bash
wally
```

Wally accepts the following options:

- `--prompt path/to/prompt.md` a custom prompt to pass to the model (defaults to `.wally/PROMPT.md` in the current dir)
- `--plan path/to/PLAN.md` custom path to the implementation plan (defaults to `.wally/PLAN.md` in the current dir)
- `--status path/to/status.json` custom path to the Wally status file (defaults to `.wally/status.json` in the current dir)
- `--agent agent-name` which opencode agent to use (defaults to `build`)
- `--model provider/model` which model to use (defaults to `opencode/grok-code`)
- `--max-loops NUMBER` the maximum number of loops to run (defaults to `50`, use `0` for infinite loops)
- `--cooldown NUMBER` number of seconds to wait between the loops (defaults to `2`)

The Wally `status.json` provides information about the current task and future task:

```json
{
    "state": "in_progress" | "completed",
    "summary": "short summary of the completed task",
    "next": "next task to be implemented"
}
```

Obviously, all of this is subject to change

## Development

To install dependencies:

```bash
bun install
```

To run:

```bash
bun run index.ts
```

This project was created using `bun init` in bun v1.3.5. [Bun](https://bun.com) is a fast all-in-one JavaScript runtime.
