# Wally

> [!WARNING]
> Almost every character in this repository has been generated by an LLM.

Wally is a robot from the future that is responsible with handling the AI
garbage.

## Overview

Wally is a very simple **orchestrator** for [opencode] for **agent invocation
with context replay**, meaning it will create a new conversation with the LLM
with a specific prompt and, once the conversation is idle, a new conversation is
created with the same prompt until a stop condition is reached.

[opencode]: https://opencode.ai/

This approach can be very useful when combining with iterative prompts. For
example:

> Analyze the `@app/models/user.rb` and `@test/models/user_test.rb` and create
> the next [characterization test][char-test] required to increase the code
> coverage for that class.
>
> You can verify the code coverage by running the following command:
>
> COVERAGE=true bin/rails test test/models/user_test.rb
>
> Once the test passes, append what you have learned to `@docs/models/user.md`.

[char-test]: https://michaelfeathers.silvrback.com/characterization-testing

The **stop condition** can be any user generated script that exits with a
non-zero condition. For example, the script can check the code coverage for the
specified file and, if the percentage is 100% or decreased from the last run,
it returns a non-zero exit code so you can inspect the output.

One of the side-effects of Wally is that it allows you to implement a loop
similar to the one described in [Geoffrey Huntley's "Ralph Wiggum as a software
engineer"][ralph] article.

[ralph]: https://ghuntley.com/ralph/

## Usage

Start a headless `opencode` server:

```bash
opencode serve
```

Then run Wally:

```bash
wally --prompt .wally/PROMPT.md --check bin/coverage
```

A more complex example:

```bash
wally --prompt .wally/PROMPT.md --check bin/coverage --limit 50 --wait 3 \
  --agent build --model xai/grok-fast-1
```

Wally _requires_ following options:

- `--prompt path/to/prompt` a custom prompt to pass to the model
- `--check path/to/script` a path to an executable (or script) that will run
  before initializing a new LLM session

Note that the `prompt` can contain references to other files (e.g. a `PLAN.md`
or a `prd.json`).

Wally also _accepts_ the following options:

- `--limit NUMBER` the maximum number of agent invocation
- `--wait NUMBER` number of seconds to wait between the loops (defaults to `3`)
- `--agent agent-name` which opencode agent to use (defaults to `build`)
- `--model provider/model` which model to use (defaults to `opencode/grok-code-fast-1`)

Obviously, all of this is subject to change.

## Development

To install dependencies:

```bash
bun install
```

To run:

```bash
bun run index.ts
```

This project was created using `bun init` in bun v1.3.5. [Bun](https://bun.com)
is a fast all-in-one JavaScript runtime.
