# Wally

> [!WARNING]
> Almost every character in this repository has been generated by an LLM.

Wally is a robot from the future that is responsible with handling the AI garbage.

## Overview

One of the more revolutionary ideas involving LLMs was [Geoffrey Huntley's "Ralph Wiggum as a software engineer"][ralph]
approach, which boils down to running the following process in a loop:

1. The LLM is given a task from an implementation plan
2. The LLM implements the task
3. The LLM updates the implementation plan

The loop stops when there are no tasks for the LLM to implement.

This approach can be very useful when working with a piece of software that does not have good test coverage.
In this case, the loop will become:

1. Implement the next [characterization test][char-test] based on a list of tests to be implemented
2. Run tests using a code coverage tool like [SimpleCov] or [mutant]
3. Update the list of tests to be done

[ralph]: https://ghuntley.com/ralph/
[char-test]: https://michaelfeathers.silvrback.com/characterization-testing
[SimpleCov]: https://github.com/simplecov-ruby/simplecov
[mutant]: https://github.com/mbj/mutant/tree/main

The original implementation used a basic bash loop, but more modern AI agents like [opencode] use a client-server
architecture which allows more fine-grained control over an API. So, in a sense, `Wally` is an orchestrator
for `opencode`.

[opencode]: https://opencode.ai/

## Development

To install dependencies:

```bash
bun install
```

To run:

```bash
bun run index.ts
```

This project was created using `bun init` in bun v1.3.5. [Bun](https://bun.com) is a fast all-in-one JavaScript runtime.
